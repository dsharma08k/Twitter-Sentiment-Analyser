# -*- coding: utf-8 -*-
"""Another copy of Twitter Sentiment Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14MXIF5E-eT36WMynB8zNVeUKnzLdYb0A
"""

!pip install kaggle

"""Uploading kaggle.json file"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""Importing Twitter Sentiment Dataset"""

!kaggle datasets download -d kazanova/sentiment140

from zipfile import ZipFile
dataset = "/content/sentiment140.zip"

with ZipFile(dataset, 'r') as zip:
  zip.extractall()
  print('done')

"""Import Dependencies"""

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV

import nltk
nltk.download('stopwords')

#Printing Stopwords
print(stopwords.words('english'))

"""Data Processing"""

#Loading csv data to pandas dataframe
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1')

#Checking the number of rows and columns
twitter_data.shape

#Printing the first 5 rows of the dataset
twitter_data.head()

# Naming the columns and reading the dataset again
column_names = ['target', 'id', 'date', 'flag', 'user', 'text']
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', names=column_names)

twitter_data.shape

twitter_data.head()

#Counting the number of missing values in the dataset
twitter_data.isnull().sum()

#Checking the distribution of target column
twitter_data['target'].value_counts()

twitter_data.replace({'target': {4:1}}, inplace=True)

"""0 - Negative
1 - Positive

Performing Stemming
"""

port_stem = PorterStemmer()

def preprocess_text(content):
    # Remove URLs, mentions, hashtags, special characters, and digits
    content = re.sub(r'https?://\\S+|www\\.\\S+', '', content)  # URLs
    content = re.sub(r'@[A-Za-z0-9_]+', '', content)           # Mentions
    content = re.sub(r'#', '', content)                        # Hashtags
    content = re.sub(r'[^a-zA-Z]', ' ', content)               # Special characters and digits
    content = content.lower()                                  # Lowercase
    content = content.split()                                  # Tokenize
    # Stem and remove stopwords
    port_stem = PorterStemmer()
    content = [port_stem.stem(word) for word in content if word not in stopwords.words('english')]
    return ' '.join(content)

twitter_data['stemmed_content'] = twitter_data['text'].apply(preprocess_text)

print(twitter_data['stemmed_content'])

print(twitter_data['target'])

#Seprating the data and label
X = twitter_data['stemmed_content'].values
Y = twitter_data['target'].values

print(X)

print(Y)

"""Spiliting the data to training and testing data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

print(X_train)

print(X_test)

"""Converting the textual data to numerical data"""

vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.8)
X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

# Model Training with Hyperparameter Tuning
param_grid = {
    'C': [0.1, 1, 10, 100],
    'solver': ['lbfgs', 'liblinear']
}

print(X_train)

print(X_test)

"""Training the ML Model using Logistic Regression"""

model = LogisticRegression(max_iter=1000)
grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, Y_train)

best_model = grid_search.best_estimator_

model.fit(X_train, Y_train)

"""Model Evaluation (Accuracy Score)

Accuracy on Training Data
"""

X_train_prediction = best_model.predict(X_train)
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)

print('Accuracy on Training Data: ', training_data_accuracy)

"""Accuracy of Test Data"""

X_test_prediction = best_model.predict(X_test)
test_data_accuracy = accuracy_score(Y_test, X_test_prediction)

print('Accuracy on Test Data: ', test_data_accuracy)

"""Saving the Trained Model"""

import pickle

pickle.dump(best_model, open('trained_model.sav', 'wb'))

pickle.dump(vectorizer, open('vectorizer.sav', 'wb'))

"""Using the saved model for prediction"""

loaded_model = pickle.load(open('trained_model.sav', 'rb'))
new_vectorizer = pickle.load(open('vectorizer.sav', 'rb'))

X_new = X_test[200]
print(Y_test[200])
prediction = loaded_model.predict(X_new)
print(prediction)
if(prediction == 0):
  print("Negative Tweet")
else:
  print("Positive Tweet")

X_new = X_test[200]
print(Y_test[200])
prediction = loaded_model.predict(X_new)
print(prediction)
if(prediction == 0):
  print("Negative Tweet")
else:
  print("Positive Tweet")

